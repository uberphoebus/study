{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d772e25-4dc3-4fa7-91f9-21fc54921911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e46fe2ba-47c5-441e-aa4f-65e8bb71952a",
   "metadata": {},
   "source": [
    "# 크롤링\n",
    "- 오늘 날짜부터 -n일차 까지 다음뉴스의 데이터를 긁어 오는 작업으로 시작합니다.\n",
    "\n",
    "- 날짜를 거꾸로 수집을 합니다.\n",
    "\n",
    "- 오늘로부터 며칠 전까지 이런식으로\n",
    "\n",
    "- 다음 랭킹뉴스에서 50개까지 url을 긁어온 뒤\n",
    "\n",
    "- 다시 for문을 이용해서 url을 접속해서 content를 긁어서 저장합니다.\n",
    "\n",
    "- 이 함수의 역할은 여기까지입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b62040e6-729b-47c8-a06a-d8a642696f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "539b8f4f-6597-4e84-8fc3-d97b28f23d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하단 코드 구동 시 './datasets/craw/다음뉴스종합.csv' 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7887dbab-ae30-41c4-ac52-52e5ca89aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def daum(dates):\n",
    "\n",
    "#     # 다음뉴스 헤드라인 긁어오기\n",
    "#     files = pd.DataFrame()\n",
    "#     for date in dates:\n",
    "#         http=[]\n",
    "#         print(date,'', 'Daum 접속 중')\n",
    "#         httz = 'https://media.daum.net/ranking/popular/?regDate={}'.format(date)\n",
    "#         res = requests.get(httz)\n",
    "#         soup = BeautifulSoup(res.content, 'html.parser')\n",
    "#         body = soup.select('#mArticle > div.rank_news > ul.list_news2')\n",
    "#         body = body[0].find_all('a')\n",
    "\n",
    "\n",
    "#         for i in range(len(body)):\n",
    "#             t = body[i].get('href')\n",
    "#             http.append(t)\n",
    "\n",
    "#         # 중복제거\n",
    "#         http = list(set(http))\n",
    "        \n",
    "#         for i in range(len(http)):\n",
    "#             res = requests.get(http[i])\n",
    "#             soup = BeautifulSoup(res.content, 'html.parser')\n",
    "#             body = soup.select('.article_view')[0]\n",
    "\n",
    "#             files = files.append(pd.DataFrame({\n",
    "#                 'date':date,\n",
    "#                 'title': soup.find('div', attrs={'class': 'head_view'}).h3.text,\n",
    "#                 'content': \" \".join(p.get_text() for p in body.find_all('p')),\n",
    "#                 'link': http[i]\n",
    "#             }, index=[i]))\n",
    "#         time.sleep(15)\n",
    "\n",
    "#         # 텍스트파일에 댓글 저장하기\n",
    "#     files.to_csv('./datasets/craw/다음뉴스종합.csv',index=False,encoding='utf-8')\n",
    "#     print('파일 저장 완료!')\n",
    "\n",
    "# ----------------- 크롤링 실행 --------------------------\n",
    "# # 현재로 부터 며칠 전까지 수집할 것인지 확인\n",
    "# days = 1\n",
    "# dates = [int(datetime.today().strftime('%Y%m%d')) - i for i in range(days)]\n",
    "# daum(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5557a156-2136-4ba0-bc18-2f9d0451a883",
   "metadata": {},
   "source": [
    "# 데이터 수집 끝\n",
    "# 뉴스 content의 내용을 tfidf를 이용하여 유사도를 분석합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e16b84c-c1f1-419d-b887-31fbb30652d3",
   "metadata": {},
   "source": [
    "<pre>\n",
    "tfidf를 모르시면 한번 확인하시길 바랍니다.\n",
    "이후 코드입니다.\n",
    "원래는 뉴스데이터 원본을 바로 Tf-idf를 이용하고, 바로 코사인유사도를 이용하여 분석하였습니다.\n",
    "하지만 조금 변경해서 명사추출로 먼저 데이터를 정제한 후에 모델에 입력합니다.\n",
    "그 이후에 Tf-idf -> 코사인유사도로 문서간의 유사도를 분석합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cdf7deb-1d86-4c95-8118-ca4cd68dc5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self의 의미\n",
    "# https://velog.io/@magnoliarfsit/RePython-1.-self-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1a9f6c6-4da7-47fa-9317-ad745329063c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>- -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name\n",
       "0   --\n",
       "1  - -\n",
       "2  NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "dfsss = pd.DataFrame({\"name\": ['',' ',np.nan]})\n",
    "dfsss['name'] = '-'+dfsss['name']+'-' \n",
    "dfsss.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c8be348-dcbb-49f6-8c7c-deb667898795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install customized-konlpy   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b6cf2e2b-d06a-49eb-ac05-e6eed59d699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from ckonlpy.tag import Twitter\n",
    "\n",
    "#dataframe에 null값이 있는 경우 공백을 넣어 null값 제거\n",
    "class TFIDF:\n",
    "    def __init__(self,df, vo):\n",
    "        self.okky_data_df = df\n",
    "        self.word_dictionary = vo\n",
    "        self.okky_data_df['noun_content'] = self.noun_extraction(self.okky_data_df['content'],\n",
    "                                                         self.word_dictionary)\n",
    "\n",
    "    # 전처리작업 (명사추출)\n",
    "    def noun_extraction(self, multi_lines ,word_dictionary):\n",
    "        tw = Twitter()\n",
    "        tw.add_dictionary(word_dictionary, 'Noun')\n",
    "\n",
    "        tokens_list = []\n",
    "\n",
    "        for idx in range(len(multi_lines)):\n",
    "            tokens_str = ''\n",
    "            # multi_lines[0] : '안녕하세요 아무개씨' \n",
    "            # multi_lines[1] : '오늘 춥다' \n",
    "            # noun_list = ['안녕', '아무개']\n",
    "            noun_list = tw.nouns(multi_lines[idx])\n",
    "            for noun_str in noun_list:                    #['안녕', '아무개']\n",
    "                tokens_str = tokens_str + ' ' + noun_str  #_안녕 아무개\n",
    "  \n",
    "            # 공백삭제\n",
    "            tokens_str = tokens_str[1:]                   #안녕 아무개\n",
    "            tokens_list.append(tokens_str)                # [\"안녕 아무개\", \"오늘\"]\n",
    "        return tokens_list\n",
    "    \n",
    "    # Null 제거 함수\n",
    "    def avoid_null(self,df, col):\n",
    "        df[col] = df[col].fillna('')\n",
    "        return df[col]\n",
    "\n",
    "    # [전제집] 유사도 행렬을 반환해주는 함수\n",
    "    def fit_tfidf(self):\n",
    "        #결측처리\n",
    "        self.okky_data_df['noun_content'] = self.avoid_null(self.okky_data_df, 'noun_content')\n",
    "        #self.okky_data_df['noun_content'].fillna('', inplace=True)\n",
    "        \n",
    "        #tf-idf계산 후 출력\n",
    "        tfidfVectorizer = TfidfVectorizer()\n",
    "        self.tfidf_metrix = tfidfVectorizer.fit_transform(self.okky_data_df['noun_content'])\n",
    "        return self.tfidf_metrix\n",
    "    \n",
    "    #입력되는 train의 질문과 질문 데이터셋의 코사인유사도 값 중 상위 50개 질문목록을 가져오는 함수\n",
    "    def top10_indices(self, dm, idx):\n",
    "        #입력된 데이터의 코사인유사도 계산\n",
    "        cos_sim = linear_kernel(dm, dm)\n",
    "\n",
    "        cos_sim_idx = list(enumerate(cos_sim[idx])) \n",
    "        cos_sim_idx = sorted(cos_sim_idx, key = lambda x : x[1], reverse = True)\n",
    "        #상위 10개 항목을 가져옴\n",
    "        topN_cos_sim_score = cos_sim_idx[1:11]\n",
    "        tag_indices = [i_cos_sim[0] for i_cos_sim in topN_cos_sim_score]\n",
    "\n",
    "        return tag_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e53deb-9532-4571-9df4-147b3fdf568e",
   "metadata": {},
   "source": [
    "word_dictionary는 명사추출에 앞서서 원하는 명사를 추출하기 위해서 사전 역할을 합니다.\n",
    "\n",
    "위에서 긁어온 데이터를 다시 불러서 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "289d0fc1-d48b-4fe0-8a05-029c47843678",
   "metadata": {},
   "outputs": [],
   "source": [
    "searchstr = ['전세집']\n",
    "\n",
    "new_df = pd.read_csv(r'./datasets/craw/다음뉴스종합.csv', encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76beee68-7605-40bc-9e54-e06f0df6061a",
   "metadata": {},
   "source": [
    "데이터를 입력하면 명사 추출까지는 자동으로 입력하고\n",
    "\n",
    "그 이후 fit_tfidf 메서드를 불러서 유사도 metrix를 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae1abab8-b534-4298-9e08-a8ea8ab20dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF = TFIDF(new_df,  searchstr)\n",
    "\n",
    "searchstr_cos_matrix = TFIDF.fit_tfidf() # [전제집] 유사도 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9dc5b5ea-379b-47a1-9695-cae4502a093a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "top10_indices() missing 1 required positional argument: 'idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9144/2128391274.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mTFIDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtop10_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearchstr_cos_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: top10_indices() missing 1 required positional argument: 'idx'"
     ]
    }
   ],
   "source": [
    "TFIDF.top10_indices(searchstr_cos_matrix, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9110699d-16d1-4b8e-92f7-177fd93b7f82",
   "metadata": {},
   "source": [
    "마지막으로 코사인유사도로 순위가 나타난 문서들을 출력할 차례입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec44cb06-6181-42b3-a58e-99d7770b535b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 50\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9144/2977462108.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#질문 제목과 데이터셋의 유사도를 10위까지 가져옴\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtit_10_q\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTFIDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtop10_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearchstr_cos_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m#     print(str(i),  okky_data.loc[i],\"\\n\", tit_10_q)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'<'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"> \\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtit_10_q\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9144/643909927.py\u001b[0m in \u001b[0;36mtop10_indices\u001b[1;34m(self, dm, topN)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;31m#상위 10개 항목을 가져옴\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mtopN_cos_sim_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcos_sim_topN\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mtag_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi_cos_sim\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopN_cos_sim_score\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtag_indices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9144/643909927.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;31m#상위 10개 항목을 가져옴\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mtopN_cos_sim_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcos_sim_topN\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mtag_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi_cos_sim\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopN_cos_sim_score\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtag_indices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# 50개까지만 출력합니다.\n",
    "count = 5\n",
    "\n",
    "for i in range(len(new_df[:count])):\n",
    "    print(i, '/', len(new_df))\n",
    "    #질문 제목과 데이터셋의 유사도를 10위까지 가져옴\n",
    "    \n",
    "    \n",
    "    tit_10_q = new_df['content'].iloc[TFIDF.top10_indices(searchstr_cos_matrix, i)]\n",
    "#     print(str(i),  okky_data.loc[i],\"\\n\", tit_10_q)\n",
    "    print(str(i),'<',new_df['title'][i],\"> \\n\", tit_10_q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b05aac2-5b19-412e-957c-298672715be3",
   "metadata": {},
   "source": [
    "정리\n",
    "\n",
    "1.데이터를 크롤링한다.\n",
    "\n",
    "2.명사만 따로 추출해서 저장한다.\n",
    "\n",
    "3.Tf-idf를 이용해서 문서유사도를 구한다.(핵심 단어만 찾아낸다.)\n",
    "\n",
    "4.구한 값을 이용해서 코사인유사도로 비슷한 문서를 찾는다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
