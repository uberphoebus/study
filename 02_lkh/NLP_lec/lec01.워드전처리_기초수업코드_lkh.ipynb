{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b6106d5-b92d-4494-b7bd-798b6d5641d1",
   "metadata": {},
   "source": [
    "## 토큰화(Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "60d8e19f-7cef-4529-adcd-fe7aeb7c3011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['font.family'] = 'Malgun Gothic'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523fdae0-f0cf-41b2-b4d7-67088b125346",
   "metadata": {},
   "source": [
    "#### 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e20fc0-e0d2-4dbd-92d6-2b3070b80116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize  \n",
    "print(word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f49eef4-0bcd-41d5-873f-0ea8f4935bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['우리는', '빅데이터', '.', '인공지능', '반이자', '학생이다']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize  \n",
    "print(word_tokenize(\"우리는 빅데이터. 인공지능 반이자 학생이다\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "372b81b3-7187-4915-806b-819fc14c4c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer  \n",
    "print(WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05361bee-da70-4253-9205-4b58f8abd07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "print(text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ae0acc1-4075-4165-aaf5-2f0f92ac3207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0540c4-568d-4e18-b371-bbfc89c67faf",
   "metadata": {},
   "source": [
    "#### 문장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3513f2e3-dd47-4d77-a46b-8a0e20594c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"\"\"His barber kept his word. \n",
    "But keeping such a huge secret to himself was driving him crazy. \n",
    "Finally, the barber went up a mountain and almost to the edge of a cliff. \n",
    "He dug a hole in the midst of some reeds. \n",
    "He looked about, to make sure no one was near.\"\"\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "563d4d79-461f-4f90-847a-7c8404dfb116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['앞서 사용한 Okt 형태소 분석기와 결과가 다른 것을 볼 수 있습니다.', '각 형태소 분석기는 성능과 결과가 다르게 나오기 MR. 때문에, 형태소 분석기의 선택은 사용하고자.', '하는 필요 용도에 어떤 형태소 분석기가 가장 적절한지를 판단하고 사용하면 됩니다.', '예를 들어서 속도를 중시한다면 메캅을 사용할 수 있습니다.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"\"\"앞서 사용한 Okt 형태소 분석기와 결과가 다른 것을 볼 수 있습니다. \n",
    "각 형태소 분석기는 성능과 결과가 다르게 나오기 MR. 때문에, 형태소 분석기의 선택은 사용하고자. 하는 필요 용도에 어떤 형태소 분석기가 가장 적절한지를 판단하고 사용하면 됩니다. \n",
    "예를 들어서 속도를 중시한다면 메캅을 사용할 수 있습니다.\"\"\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04595803-f066-4222-8736-5bba54e26f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kss\n",
    "# text = \"\"\"앞서 사용한 Okt 형태소 분석기와 결과가 다른 것을 볼 수 있습니다. \n",
    "# 각 형태소 분석기는 성능과 결과가 다르게 나오기 MR. 때문에, 형태소 분석기의 선택은 사용하고자. 하는 필요 용도에 어떤 형태소 분석기가 가장 적절한지를 판단하고 사용하면 됩니다. \n",
    "# 예를 들어서 속도를 중시한다면 메캅을 사용할 수 있습니다. 해보면 아실걸요?\"\"\"\n",
    "# print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6fabe5-8342-4996-9057-ac0b36fb3775",
   "metadata": {},
   "source": [
    "### 품사 태깅\n",
    "* Konlpy\n",
    "* 1) morphs() : 형태소 추출\n",
    "* 2) pos() : 품사 태깅(Part-of-speech tagging)\n",
    "* 3) nouns() : 명사 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "548b0d18-713c-4a40-96b3-f0f8c38697b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  averaged_perceptron_tagger download\n",
    "# from nltk.tag import pos_tag\n",
    "# text = \"\"\"His barber kept his word.\"\"\"\n",
    "# tokenized_sentence = word_tokenize(text)\n",
    "# print(pos_tag(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9857ea46-f073-4d6f-bc46-97dfcd653214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('아버지', 'Noun'), ('가', 'Josa'), ('방', 'Noun'), ('에', 'Josa'), ('들어가신다', 'Verb')]\n",
      "[('아버지', 'Noun'), ('가방', 'Noun'), ('에', 'Josa'), ('들어가신다', 'Verb')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "print(okt.pos('아버지가 방에 들어가신다'))\n",
    "print(okt.pos('아버지가방에들어가신다'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9deaf54-a1c6-4b28-a30b-0569664529ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('아버지', 'NNG'), ('가', 'JKS'), ('방', 'NNG'), ('에', 'JKM'), ('들어가', 'VV'), ('시', 'EPH'), ('ㄴ다', 'EFN')]\n",
      "[('아버지', 'NNG'), ('가방', 'NNG'), ('에', 'JKM'), ('들어가', 'VV'), ('시', 'EPH'), ('ㄴ다', 'EFN')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "okt = Kkma()\n",
    "\n",
    "print(okt.pos('아버지가 방에 들어가신다'))\n",
    "print(okt.pos('아버지가방에들어가신다'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba3d9436-7ba1-487c-b343-ed0e913852fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n"
     ]
    }
   ],
   "source": [
    "# 형태소 : morphs\n",
    "from konlpy.tag import Kkma  \n",
    "kkma = Kkma()  \n",
    "print(kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "834c03f0-1781-4e1e-80e6-23a8baf260c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n"
     ]
    }
   ],
   "source": [
    "# 품사 : pos\n",
    "print(kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56f75643-b684-4ffc-bbda-9485ac64807c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "# 명사 : nouns\n",
    "print(kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd88098d-744a-41a1-8c7e-4b06bdb8cc67",
   "metadata": {},
   "source": [
    "#### [실습] 워드카운트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cb187c8e-202b-4d64-9a20-855d367de770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "df = pd.read_csv(\"../pkg/kaggle/DACON_식수/datasets/train.csv\")\n",
    "df = df[['중식메뉴','석식메뉴']][:5].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ddda8090-2657-4e0a-a685-50f46f5af3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   중식메뉴    5 non-null      object\n",
      " 1   석식메뉴    5 non-null      object\n",
      "dtypes: object(2)\n",
      "memory usage: 208.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e4b4e-20bc-4f0b-bcb3-4dbb54b42796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>중식메뉴</th>\n",
       "      <th>석식메뉴</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>쌀밥/잡곡밥 (쌀,현미흑미:국내산) 오징어찌개  쇠불고기 (쇠고기:호주산) 계란찜 ...</td>\n",
       "      <td>쌀밥/잡곡밥 (쌀,현미흑미:국내산) 육개장  자반고등어구이  두부조림  건파래무침 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>쌀밥/잡곡밥 (쌀,현미흑미:국내산) 김치찌개  가자미튀김  모둠소세지구이  마늘쫑무...</td>\n",
       "      <td>콩나물밥*양념장 (쌀,현미흑미:국내산) 어묵국  유산슬 (쇠고기:호주산) 아삭고추무...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>카레덮밥 (쌀,현미흑미:국내산) 팽이장국  치킨핑거 (닭고기:국내산) 쫄면야채무침 ...</td>\n",
       "      <td>쌀밥/잡곡밥 (쌀,현미흑미:국내산) 청국장찌개  황태양념구이 (황태:러시아산) 고기...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>쌀밥/잡곡밥 (쌀,현미흑미:국내산) 쇠고기무국  주꾸미볶음  부추전  시금치나물  ...</td>\n",
       "      <td>미니김밥*겨자장 (쌀,현미흑미:국내산) 우동  멕시칸샐러드  군고구마  무피클  포...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>쌀밥/잡곡밥 (쌀,현미흑미:국내산) 떡국  돈육씨앗강정 (돼지고기:국내산) 우엉잡채...</td>\n",
       "      <td>쌀밥/잡곡밥 (쌀,현미흑미:국내산) 차돌박이찌개 (쇠고기:호주산) 닭갈비 (닭고기:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                중식메뉴  \\\n",
       "0  쌀밥/잡곡밥 (쌀,현미흑미:국내산) 오징어찌개  쇠불고기 (쇠고기:호주산) 계란찜 ...   \n",
       "1  쌀밥/잡곡밥 (쌀,현미흑미:국내산) 김치찌개  가자미튀김  모둠소세지구이  마늘쫑무...   \n",
       "2  카레덮밥 (쌀,현미흑미:국내산) 팽이장국  치킨핑거 (닭고기:국내산) 쫄면야채무침 ...   \n",
       "3  쌀밥/잡곡밥 (쌀,현미흑미:국내산) 쇠고기무국  주꾸미볶음  부추전  시금치나물  ...   \n",
       "4  쌀밥/잡곡밥 (쌀,현미흑미:국내산) 떡국  돈육씨앗강정 (돼지고기:국내산) 우엉잡채...   \n",
       "\n",
       "                                                석식메뉴  \n",
       "0  쌀밥/잡곡밥 (쌀,현미흑미:국내산) 육개장  자반고등어구이  두부조림  건파래무침 ...  \n",
       "1  콩나물밥*양념장 (쌀,현미흑미:국내산) 어묵국  유산슬 (쇠고기:호주산) 아삭고추무...  \n",
       "2  쌀밥/잡곡밥 (쌀,현미흑미:국내산) 청국장찌개  황태양념구이 (황태:러시아산) 고기...  \n",
       "3  미니김밥*겨자장 (쌀,현미흑미:국내산) 우동  멕시칸샐러드  군고구마  무피클  포...  \n",
       "4  쌀밥/잡곡밥 (쌀,현미흑미:국내산) 차돌박이찌개 (쇠고기:호주산) 닭갈비 (닭고기:...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9535f7b4-72b2-4bcd-8846-6e0026b61a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "쌀밥/잡곡밥 오징어찌개 쇠불고기 계란찜 청포묵무침 요구르트 포기김치\n",
      "쌀밥/잡곡밥 김치찌개 가자미튀김 모둠소세지구이 마늘쫑무침 요구르트 배추겉절이\n",
      "카레덮밥 팽이장국 치킨핑거 쫄면야채무침 견과류조림 요구르트 포기김치\n",
      "쌀밥/잡곡밥 쇠고기무국 주꾸미볶음 부추전 시금치나물 요구르트 포기김치\n",
      "쌀밥/잡곡밥 떡국 돈육씨앗강정 우엉잡채 청경채무침 요구르트 포기김치\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "요구르트    5\n",
       "잡곡밥     4\n",
       "포기김치    4\n",
       "쌀밥      4\n",
       "김치찌개    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_list = []\n",
    "for sen in df['중식메뉴']:\n",
    "    sen = re.sub(r'\\([^)]*\\)', '', sen.strip())  #(s) 제거\n",
    "    sen = re.sub(r'[^)]*\\)', '', sen.strip())    #s) 제거\n",
    "    sen = re.sub(r'\\([^)]*', '', sen.strip())    #(s 제거\n",
    "    sen = re.sub(' +', ' ', sen.strip())       #공백,탭 제거\n",
    "    print(sen)\n",
    "    #nous = kkma.nouns(sen)\n",
    "    nous = text_to_word_sequence(sen)\n",
    "    sen_list.append(nous)  \n",
    "sen_list = sum(sen_list, [])  #2차원-->1차원으로 변환\n",
    "pd.DataFrame(sen_list).value_counts().sort_values(ascending=False).head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deaf1c5-ac57-44e5-942e-069f8a914824",
   "metadata": {},
   "source": [
    "## 어간 추출(Stemming) and 표제어 추출(Lemmatization)\n",
    "* Lemmatization : 품사 태깅과 같이 써야 정확하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b5620826-ce30-4d25-96c7-bd5a62057c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n",
      "ha--> have\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print([lemmatizer.lemmatize(w) for w in words])\n",
    "print(\"ha-->\", lemmatizer.lemmatize('has', 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713eb9c4-20ad-4b41-9e3a-2cec0d94ce8a",
   "metadata": {},
   "source": [
    "### 불용언 처리(영어)\n",
    "* nltk에서 제공하느 stopwords 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e83a0f38-d94d-4c92-b210-3630a7f66963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
      "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "text = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = word_tokenize(text)\n",
    "result = []\n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        result.append(w) \n",
    "\n",
    "print(word_tokens) \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6f29af19-abf9-49f6-97af-85645dc71ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 0)\t1\n",
      "[[1 1 1 1]]\n",
      "{'family': 1, 'important': 2, 'thing': 3, 'everything': 0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords \n",
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "sw = set(stopwords.words('english')) \n",
    "\n",
    "# CountVectorizer(stop_words=[\"not\", \"an\"])\n",
    "cvect = CountVectorizer(stop_words=sw)\n",
    "res = cvect.fit_transform(text)\n",
    "print(res)  # COO vs. CSR\n",
    "print(res.toarray())\n",
    "print(cvect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ded8b1-bc69-49a3-bdf1-850aad2da4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    " # COO(밀집행렬) vs. CSR(희소행렬)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5224de2c-eb65-4778-a629-e95be2a2dac9",
   "metadata": {},
   "source": [
    "### 불용언 처리(한글)\n",
    "* ref : https://github.com/stopwords-iso/stopwords-ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9b5f9c07-df5c-48c9-a658-e539c3945b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '$', '%', '&', \"'\", '(', ')', '*', '+']\n"
     ]
    }
   ],
   "source": [
    "f = open(\"stopword_ko.txt\", mode='r', encoding=\"UTF-8\")\n",
    "stopword_ko_list = []\n",
    "for word in f:\n",
    "    stopword_ko_list.append(word.rstrip(\"\\n\"))\n",
    "    \n",
    "print(stopword_ko_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "797589fe-8a79-4790-b565-897ff6831c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 3)\t1\n",
      "[[1 1 1 1 1 1]]\n",
      "{'나는': 1, '훌륭한': 5, '사업가가되서': 2, '쿵쿵': 4, '거리며': 0, '살겠다': 3}\n",
      "[('훌륭한', 5), ('쿵쿵', 4), ('살겠다', 3), ('사업가가되서', 2), ('나는', 1), ('거리며', 0)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords \n",
    "text = [\"나는 개의치않고 언젠가 훌륭한 사업가가되서 쿵쿵 거리며 살겠다.\"]\n",
    "# sw = set(stopwords.words('english'))   XXXXXX\n",
    "\n",
    "cvect = CountVectorizer(stop_words=stopword_ko_list)\n",
    "res = cvect.fit_transform(text)\n",
    "print(res)  # COO vs. CSR\n",
    "print(res.toarray())\n",
    "print(cvect.vocabulary_)\n",
    "\n",
    "# key = [(빅분기, 4) , (파이썬, 5)]\n",
    "# for x in key:\n",
    "#     x = (빅분기, 4)\n",
    "    \n",
    "vocab_sorted = sorted(cvect.vocabulary_.items(), key = lambda x:x[1], reverse = True)\n",
    "print(vocab_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594d1a4a-2828-4fac-b3c5-9674ae4fae2f",
   "metadata": {},
   "source": [
    "* Keras 사용한 빈도수 보캐 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6b030bb9-6067-496c-8c4b-b9d56df713de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'driving': 10, 'crazy': 11, 'went': 12, 'mountain': 13}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "text = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
    "tokenizer = Tokenizer()\n",
    "# fit_on_texts()안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성.\n",
    "tokenizer.fit_on_texts(text) \n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48d7a54-c324-4290-ae40-1c8a0081e966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76243fd2-0958-4abd-a140-7231d88c29a3",
   "metadata": {},
   "source": [
    "## 패딩(padding)\n",
    "* Tokenizer.fit_on_texts()  - texts_to_sequences() - pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ddb6c3ea-a88e-42aa-a0b5-b31849d8d457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  5  0  0  0]\n",
      " [ 1  8  5  0  0]\n",
      " [ 1  3  5  0  0]\n",
      " [ 9  2  0  0  0]\n",
      " [ 2  4  3  2  0]\n",
      " [ 3  2  0  0  0]\n",
      " [ 1  4  6  0  0]\n",
      " [ 1  4  6  0  0]\n",
      " [ 1  4  2  0  0]\n",
      " [ 3  2 10  1 11]\n",
      " [ 1 12  3 13  0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)                  #단어 빈도수 보캐 생성\n",
    "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)  #정수변환\n",
    "padded = pad_sequences(encoded, padding = 'post', maxlen = 5)  #pre or post\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48532ea-ea63-4dc6-bde2-819476194c0a",
   "metadata": {},
   "source": [
    "* keras 원핫인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "10bad4ec-e60c-4f67-8a76-803985b33420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "text = \"나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야\"\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])   # 빈도 기반 보캐 생성 시 데이터는 반드시 !!! 리스트 타입\n",
    "print(tokenizer.word_index)      # 각 단어에 대한 인코딩 결과 출력."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1109d969-465a-4183-bfbf-0b4152a39310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}\n",
      "[2, 5, 1, 6, 3, 7]\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "sub_text = \"점심 먹으러 갈래 메뉴는 햄버거 최고야\"\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])   # 빈도 기반 보캐 생성 시 데이터는 반드시 !!! 리스트 타입\n",
    "print(tokenizer.word_index)      # 각 단어에 대한 인코딩 결과 출력.\n",
    "encoded = tokenizer.texts_to_sequences([sub_text])[0]\n",
    "print(encoded)\n",
    "one_hot = to_categorical(encoded)\n",
    "print(one_hot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
