{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d772e25-4dc3-4fa7-91f9-21fc54921911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e46fe2ba-47c5-441e-aa4f-65e8bb71952a",
   "metadata": {},
   "source": [
    "# 크롤링\n",
    "- 오늘 날짜부터 -n일차 까지 다음뉴스의 데이터를 긁어 오는 작업으로 시작합니다.\n",
    "\n",
    "- 날짜를 거꾸로 수집을 합니다.\n",
    "\n",
    "- 오늘로부터 며칠 전까지 이런식으로\n",
    "\n",
    "- 다음 랭킹뉴스에서 50개까지 url을 긁어온 뒤\n",
    "\n",
    "- 다시 for문을 이용해서 url을 접속해서 content를 긁어서 저장합니다.\n",
    "\n",
    "- 이 함수의 역할은 여기까지입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b62040e6-729b-47c8-a06a-d8a642696f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "539b8f4f-6597-4e84-8fc3-d97b28f23d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하단 코드 구동 시 './datasets/craw/다음뉴스종합.csv' 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7887dbab-ae30-41c4-ac52-52e5ca89aaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def daum(dates):\n",
    "\n",
    "#     # 다음뉴스 헤드라인 긁어오기\n",
    "#     files = pd.DataFrame()\n",
    "#     for date in dates:\n",
    "#         http=[]\n",
    "#         print(date,'', 'Daum 접속 중')\n",
    "#         httz = 'https://media.daum.net/ranking/popular/?regDate={}'.format(date)\n",
    "#         res = requests.get(httz)\n",
    "#         soup = BeautifulSoup(res.content, 'html.parser')\n",
    "#         body = soup.select('#mArticle > div.rank_news > ul.list_news2')\n",
    "#         body = body[0].find_all('a')\n",
    "\n",
    "\n",
    "#         for i in range(len(body)):\n",
    "#             t = body[i].get('href')\n",
    "#             http.append(t)\n",
    "\n",
    "#         # 중복제거\n",
    "#         http = list(set(http))\n",
    "        \n",
    "#         for i in range(len(http)):\n",
    "#             res = requests.get(http[i])\n",
    "#             soup = BeautifulSoup(res.content, 'html.parser')\n",
    "#             body = soup.select('.article_view')[0]\n",
    "\n",
    "#             files = files.append(pd.DataFrame({\n",
    "#                 'date':date,\n",
    "#                 'title': soup.find('div', attrs={'class': 'head_view'}).h3.text,\n",
    "#                 'content': \" \".join(p.get_text() for p in body.find_all('p')),\n",
    "#                 'link': http[i]\n",
    "#             }, index=[i]))\n",
    "#         time.sleep(15)\n",
    "\n",
    "#         # 텍스트파일에 댓글 저장하기\n",
    "#     files.to_csv('./datasets/craw/다음뉴스종합.csv',index=False,encoding='utf-8')\n",
    "#     print('파일 저장 완료!')\n",
    "\n",
    "# ----------------- 크롤링 실행 --------------------------\n",
    "# # 현재로 부터 며칠 전까지 수집할 것인지 확인\n",
    "# days = 1\n",
    "# dates = [int(datetime.today().strftime('%Y%m%d')) - i for i in range(days)]\n",
    "# daum(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5557a156-2136-4ba0-bc18-2f9d0451a883",
   "metadata": {},
   "source": [
    "# 데이터 수집 끝\n",
    "# 뉴스 content의 내용을 tfidf를 이용하여 유사도를 분석합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e16b84c-c1f1-419d-b887-31fbb30652d3",
   "metadata": {},
   "source": [
    "<pre>\n",
    "tfidf를 모르시면 한번 확인하시길 바랍니다.\n",
    "이후 코드입니다.\n",
    "원래는 뉴스데이터 원본을 바로 Tf-idf를 이용하고, 바로 코사인유사도를 이용하여 분석하였습니다.\n",
    "하지만 조금 변경해서 명사추출로 먼저 데이터를 정제한 후에 모델에 입력합니다.\n",
    "그 이후에 Tf-idf -> 코사인유사도로 문서간의 유사도를 분석합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cdf7deb-1d86-4c95-8118-ca4cd68dc5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self의 의미\n",
    "# https://velog.io/@magnoliarfsit/RePython-1.-self-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1a9f6c6-4da7-47fa-9317-ad745329063c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>- -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name\n",
       "0   --\n",
       "1  - -\n",
       "2  NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "dfsss = pd.DataFrame({\"name\": ['',' ',np.nan]})\n",
    "dfsss['name'] = '-'+dfsss['name']+'-' \n",
    "dfsss.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c8be348-dcbb-49f6-8c7c-deb667898795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install customized-konlpy   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6cf2e2b-d06a-49eb-ac05-e6eed59d699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from ckonlpy.tag import Twitter\n",
    "\n",
    "#dataframe에 null값이 있는 경우 공백을 넣어 null값 제거\n",
    "class TFIDF:\n",
    "    def __init__(self,df, vo):\n",
    "        self.okky_data_df = df\n",
    "        self.word_dictionary = vo\n",
    "        self.okky_data_df['noun_content'] = self.noun_extraction(self.okky_data_df['content'],\n",
    "                                                         self.word_dictionary)\n",
    "\n",
    "    # 전처리작업 (명사추출)\n",
    "    def noun_extraction(self, multi_lines ,word_dictionary):\n",
    "        tw = Twitter()\n",
    "        tw.add_dictionary(word_dictionary, 'Noun')\n",
    "\n",
    "        tokens_list = []\n",
    "\n",
    "        for idx in range(len(multi_lines)):\n",
    "            tokens_str = ''\n",
    "            # multi_lines[0] : '안녕하세요 아무개씨' \n",
    "            # multi_lines[1] : '오늘 춥다' \n",
    "            # noun_list = ['안녕', '아무개']\n",
    "            noun_list = tw.nouns(multi_lines[idx])\n",
    "            for noun_str in noun_list:                    #['안녕', '아무개']\n",
    "                tokens_str = tokens_str + ' ' + noun_str  #_안녕 아무개\n",
    "  \n",
    "            # 공백삭제\n",
    "            tokens_str = tokens_str[1:]                   #안녕 아무개\n",
    "            tokens_list.append(tokens_str)                # [\"안녕 아무개\", \"오늘\"]\n",
    "        return tokens_list\n",
    "    \n",
    "    # Null 제거 함수\n",
    "    def avoid_null(self,df, col):\n",
    "        df[col] = df[col].fillna('')\n",
    "        return df[col]\n",
    "\n",
    "    # [전제집] 유사도 행렬을 반환해주는 함수\n",
    "    def fit_tfidf(self):\n",
    "        #결측처리\n",
    "        self.okky_data_df['noun_content'] = self.avoid_null(self.okky_data_df, 'noun_content')\n",
    "        #self.okky_data_df['noun_content'].fillna('', inplace=True)\n",
    "        \n",
    "        #tf-idf계산 후 출력\n",
    "        tfidfVectorizer = TfidfVectorizer()\n",
    "        self.tfidf_metrix = tfidfVectorizer.fit_transform(self.okky_data_df['noun_content'])\n",
    "        return self.tfidf_metrix\n",
    "    \n",
    "    #입력되는 train의 질문과 질문 데이터셋의 코사인유사도 값 중 상위 50개 질문목록을 가져오는 함수\n",
    "    def top10_indices(self, dm, idx):\n",
    "        #입력된 데이터의 코사인유사도 계산\n",
    "        cos_sim = linear_kernel(dm, dm)\n",
    "\n",
    "        cos_sim_idx = list(enumerate(cos_sim[idx])) \n",
    "        cos_sim_idx = sorted(cos_sim_idx, key = lambda x : x[1], reverse = True)\n",
    "        #상위 10개 항목을 가져옴\n",
    "        topN_cos_sim_score = cos_sim_idx[1:11]\n",
    "        tag_indices = [i_cos_sim[0] for i_cos_sim in topN_cos_sim_score]\n",
    "\n",
    "        return tag_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e53deb-9532-4571-9df4-147b3fdf568e",
   "metadata": {},
   "source": [
    "word_dictionary는 명사추출에 앞서서 원하는 명사를 추출하기 위해서 사전 역할을 합니다.\n",
    "\n",
    "위에서 긁어온 데이터를 다시 불러서 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "289d0fc1-d48b-4fe0-8a05-029c47843678",
   "metadata": {},
   "outputs": [],
   "source": [
    "searchstr = ['전세집']\n",
    "\n",
    "new_df = pd.read_csv(r'./datasets/craw/다음뉴스종합.csv', encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76beee68-7605-40bc-9e54-e06f0df6061a",
   "metadata": {},
   "source": [
    "데이터를 입력하면 명사 추출까지는 자동으로 입력하고\n",
    "\n",
    "그 이후 fit_tfidf 메서드를 불러서 유사도 metrix를 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae1abab8-b534-4298-9e08-a8ea8ab20dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ai\\pythonproject\\venv\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "TFIDF = TFIDF(new_df,  searchstr)\n",
    "\n",
    "searchstr_cos_matrix = TFIDF.fit_tfidf() # [전제집] 유사도 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dc5b5ea-379b-47a1-9695-cae4502a093a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 39, 28, 38, 24, 44, 43, 12, 11, 36]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF.top10_indices(searchstr_cos_matrix, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9110699d-16d1-4b8e-92f7-177fd93b7f82",
   "metadata": {},
   "source": [
    "마지막으로 코사인유사도로 순위가 나타난 문서들을 출력할 차례입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec44cb06-6181-42b3-a58e-99d7770b535b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 50\n",
      "0 < 돈도 축하도 필요없다..오늘 혼인 신고하고 왕실떠난 日공주 > \n",
      " 9     왕이 중국 외교담당 국무위원 겸 외교부장이 일본을 향해 “역사 인식과 대만 등은 중...\n",
      "39     (서울=뉴스1) 김일창 기자 = 노태우 전 대통령이 26일 지병 악화로 서울 종로...\n",
      "28     초기에 발견하기 어렵기 때문에 '침묵의 살인자'라 불리는 췌장암에 걸리는 젊은 여...\n",
      "38     (서울=뉴스1) 김규빈 기자,음상준 기자,권영미 기자,이형진 기자,강승지 기자 =...\n",
      "24      1970년대 중반의 일이다. 당시 공수여단장이었던 노태우 전 대통령은 난생 처음...\n",
      "44     이인제 전 국회의원이 더불어민주당 대선 후보로 이재명 경기도지사가 선출된 것과 관...\n",
      "43     [이데일리 박지혜 기자] 대한민국 제13대 대통령을 지낸 노태우 전 대통령이 26...\n",
      "12     (시카고=연합뉴스) 김현 통신원 = 미국에서 많은 의문을 남긴 채 실종됐던 전도유...\n",
      "11     [아시아경제 박준이 기자] 이재명 더불어민주당 대선 후보와 홍준표 국민의힘 의원 ...\n",
      "36      # 내년 4월 결혼식을 위해 웨딩홀을 예약하려던 서울 서초동의 김모(32)씨. ...\n",
      "Name: content, dtype: object\n",
      "1 / 50\n",
      "1 < '구속 갈림길' 영장심사 마치고 법원 나서는 손준성 검사 [TF사진관] > \n",
      " 37     (서울=뉴스1) 한유주 기자,최현만 기자 = 고발사주 의혹의 핵심인물이자 텔레그램...\n",
      "27     김용판 의원은 이날 '돈다발' 사진과 관련해 \"민주당은 돈다발 사진이 박철민 페이...\n",
      "35     (서울=뉴스1) 이훈철 기자,김민성 기자 = 윤석열 국민의힘 대선 경선 후보가 2...\n",
      "49     (서울=뉴스1) 허고운 기자,이밝음 기자 = 26일 국회 행정안전위원회 종합감사에...\n",
      "48    기사내용 요약 공수처의 손준성 전 정책관 구속영장 청구 관련  변협 \"방어권 보장 ...\n",
      "33     윤석열 전 검찰총장의 이재명 더불어민주당 대선후보와 양자대결 경쟁력이 국민의힘 대...\n",
      "44     이인제 전 국회의원이 더불어민주당 대선 후보로 이재명 경기도지사가 선출된 것과 관...\n",
      "41     국민의힘 대선 후보를 결정하기 위한 본경선 여론조사 문항이 결정됐다. '가상 양자...\n",
      "11     [아시아경제 박준이 기자] 이재명 더불어민주당 대선 후보와 홍준표 국민의힘 의원 ...\n",
      "46    기사내용 요약 검찰, 김만배 남욱 소환 조사 이어가 '박영수 딸 특혜\" 질문에 김만...\n",
      "Name: content, dtype: object\n",
      "2 / 50\n",
      "2 < 김정숙 여사 '그때 그 가방'..에르메스 아닌 60만원대 '국산백' > \n",
      " 30     독립운동가 안중근 의사의 조카며느리 고(故) 박태정 여사가 생활고로 삼일장도 치르...\n",
      "39     (서울=뉴스1) 김일창 기자 = 노태우 전 대통령이 26일 지병 악화로 서울 종로...\n",
      "35     (서울=뉴스1) 이훈철 기자,김민성 기자 = 윤석열 국민의힘 대선 경선 후보가 2...\n",
      "29     노태우 전 대통령이 26일 별세했다. 향년 89세. 이날은 박정희 전 대통령의 기...\n",
      "36      # 내년 4월 결혼식을 위해 웨딩홀을 예약하려던 서울 서초동의 김모(32)씨. ...\n",
      "24      1970년대 중반의 일이다. 당시 공수여단장이었던 노태우 전 대통령은 난생 처음...\n",
      "12     (시카고=연합뉴스) 김현 통신원 = 미국에서 많은 의문을 남긴 채 실종됐던 전도유...\n",
      "6      지난 21일 누리호가 우주로 향할 준비를 하고 있을 무렵. 발사대에서 20km 떨...\n",
      "42    [곽우신 기자]  윤석열 후보는 박정희 전 대통령 사망 42주기를 맞아 26일 오후...\n",
      "28     초기에 발견하기 어렵기 때문에 '침묵의 살인자'라 불리는 췌장암에 걸리는 젊은 여...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 50개까지만 출력합니다.\n",
    "count = 3\n",
    "\n",
    "for i in range(len(new_df[:count])):\n",
    "    print(i, '/', len(new_df))\n",
    "    #질문 제목과 데이터셋의 유사도를 10위까지 가져옴\n",
    "    \n",
    "    \n",
    "    tit_10_q = new_df['content'].iloc[TFIDF.top10_indices(searchstr_cos_matrix, i)]\n",
    "#     print(str(i),  okky_data.loc[i],\"\\n\", tit_10_q)\n",
    "    print(str(i),'<',new_df['title'][i],\"> \\n\", tit_10_q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b05aac2-5b19-412e-957c-298672715be3",
   "metadata": {},
   "source": [
    "정리\n",
    "\n",
    "1.데이터를 크롤링한다.\n",
    "\n",
    "2.명사만 따로 추출해서 저장한다.\n",
    "\n",
    "3.Tf-idf를 이용해서 문서유사도를 구한다.(핵심 단어만 찾아낸다.)\n",
    "\n",
    "4.구한 값을 이용해서 코사인유사도로 비슷한 문서를 찾는다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
